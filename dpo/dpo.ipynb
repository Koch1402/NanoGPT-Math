{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "124a869a",
      "metadata": {
        "id": "124a869a"
      },
      "source": [
        "### Step 1: Install necesscary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b82f8f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b82f8f1",
        "outputId": "30136c18-c27c-4545-bae5-fe87cfd78dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib\n",
        "%pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "086be04f"
      },
      "source": [
        "### Specify the path to your `model` module\n",
        "Please replace the placeholder path below with the actual path to the directory containing your `model.py` file."
      ],
      "id": "086be04f"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJoAp02OG43y",
        "outputId": "941b1622-c8c5-4d4e-9697-3926fc240491"
      },
      "id": "mJoAp02OG43y",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84551f30",
        "outputId": "1e93625f-d97d-46d6-faf6-78a003eaed67"
      },
      "source": [
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/SC3000\" #@param {type:\"string\"}\n",
        "\n",
        "import sys\n",
        "import os\n",
        "if MODEL_DIR not in sys.path:\n",
        "    sys.path.append(MODEL_DIR)\n",
        "\n",
        "print(f\"Added {MODEL_DIR} to sys.path\")"
      ],
      "id": "84551f30",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added /content/drive/MyDrive/SC3000 to sys.path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c2d9de0",
      "metadata": {
        "id": "6c2d9de0"
      },
      "source": [
        "### Step 2: Package imports and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "876dd92d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "876dd92d",
        "outputId": "a8642e2a-1dbf-4719-a311-f48988f591d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "# The following line is commented out as it might be redundant if MODEL_DIR is set correctly\n",
        "# sys.path.append(os.path.abspath(\"..\"))\n",
        "# Setting CUDA_VISIBLE_DEVICES is typically not needed in Colab\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import pickle\n",
        "from model import GPT, GPTConfig\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "# Configuration\n",
        "beta = 0.5\n",
        "# Use 'cuda' if available, otherwise 'mps' if available, otherwise 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "base_lr = 1e-4\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "max_length = 64\n",
        "num_samples = 1\n",
        "max_new_tokens = 200\n",
        "temperature = 0.8\n",
        "top_k = 200\n",
        "print(device)\n",
        "# tokenizer\n",
        "tokenizer_path = os.path.join(\"/content/drive/MyDrive/SC3000/sft\", \"meta.pkl\")\n",
        "with open(tokenizer_path, \"rb\") as f:\n",
        "    meta = pickle.load(f)\n",
        "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7d35e6",
      "metadata": {
        "id": "4c7d35e6"
      },
      "source": [
        "### Step 3: Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d03655c3",
      "metadata": {
        "id": "d03655c3"
      },
      "outputs": [],
      "source": [
        "def compute_logprob(input_ids):\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "    logits, _ = gpt(inputs, full_seq=True)\n",
        "    B, T, V = logits.size()\n",
        "    logits_flat = logits.reshape(-1, V)\n",
        "    targets_flat = targets.reshape(-1)\n",
        "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
        "    loss = loss.reshape(B, T)\n",
        "    attention_mask = (targets != 0).float()\n",
        "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
        "    return -loss\n",
        "\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
        "\n",
        "def get_batches(lines, batch_size):\n",
        "    random.shuffle(lines)\n",
        "    #for l in lines:\n",
        "    #    print(l[1])\n",
        "    for i in range(0, len(lines), batch_size):\n",
        "        batch = lines[i:i+batch_size]\n",
        "        if len(batch) < batch_size:\n",
        "            continue\n",
        "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
        "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
        "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
        "        yield neg_tensor, pos_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9d9eba",
      "metadata": {
        "id": "fc9d9eba"
      },
      "source": [
        "### Step 4: Load the pretrained NanoGPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ceae772a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceae772a",
        "outputId": "1c670085-6877-4974-db4f-ac59ea2adc7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(74, 348)\n",
              "    (wpe): Embedding(256, 348)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
              "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ckpt = torch.load(\"/content/drive/MyDrive/SC3000/sft/gpt.pt\", map_location=device)\n",
        "gptconf = GPTConfig(**ckpt['model_args'])\n",
        "gpt = GPT(gptconf)\n",
        "state_dict = ckpt['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k in list(state_dict.keys()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "gpt.to(device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0feafc5a",
      "metadata": {
        "id": "0feafc5a"
      },
      "source": [
        "### Step 5: Load Data (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7edf3d44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7edf3d44",
        "outputId": "079be3f7-9d89-4ca0-a447-c2b8e8a49c7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100000 samples\n",
            "Example first entry: {'positive': '16+37=? The answer is 53 because 16+37 equals 53.', 'negative': \"16+37=? Sorry,I don't know!\"}\n",
            "Example after cleaning: {'positive': '16+37=? The answer is 53 because 16+37 equals 53.', 'negative': \"16+37=? Sorry,I don't know\"}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def load_jsonl_as_list(path: str | Path):\n",
        "    path = Path(path)\n",
        "    records = []\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            s = line.strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            try:\n",
        "                obj = json.loads(s)\n",
        "            except json.JSONDecodeError as e:\n",
        "                raise RuntimeError(f\"Bad JSON on line {i}: {e}\") from e\n",
        "            if not (isinstance(obj, dict) and \"positive\" in obj and \"negative\" in obj):\n",
        "                raise ValueError(f\"Bad schema on line {i}: {obj}\")\n",
        "            records.append(obj)\n",
        "    if not records:\n",
        "        raise RuntimeError(f\"No records found in {path}\")\n",
        "    return records\n",
        "\n",
        "# --- use it just like your old code ---\n",
        "lines = load_jsonl_as_list(\"/content/drive/MyDrive/SC3000/data/pos_neg_pairs.jsonl\")  # <-- JSONL file\n",
        "\n",
        "print(f\"Loaded {len(lines)} samples\")\n",
        "print(\"Example first entry:\", lines[0])\n",
        "\n",
        "def clean_sample(sample, stoi):\n",
        "    sample['negative'] = ''.join([c for c in sample['negative'] if c in stoi])\n",
        "    sample['positive'] = ''.join([c for c in sample['positive'] if c in stoi])\n",
        "    return sample\n",
        "\n",
        "# keep your cleaning step unchanged\n",
        "lines = [clean_sample(p, stoi) for p in lines]\n",
        "\n",
        "print(\"Example after cleaning:\", lines[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e5f81f",
      "metadata": {
        "id": "c2e5f81f"
      },
      "source": [
        "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "df0c400f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df0c400f",
        "outputId": "e765265f-5e3a-4483-e1b1-ce0e6a946ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdamW ready | fused=False\n",
            "steps/epoch=1562, total_steps=7810, warmup_steps=234\n"
          ]
        }
      ],
      "source": [
        "# ===== AdamW + Smooth Cosine (cosine warmup + cosine decay) =====\n",
        "import math\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "def build_param_groups(model, weight_decay=0.1):\n",
        "    decay, no_decay = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        (decay if p.dim() >= 2 else no_decay).append(p)\n",
        "    return [\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "def build_adamw(model, lr, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-8):\n",
        "    fused_ok = (\n",
        "        device == \"cuda\"\n",
        "        and hasattr(torch.optim, \"AdamW\")\n",
        "        and \"fused\" in AdamW.__init__.__code__.co_varnames\n",
        "    )\n",
        "    return AdamW(build_param_groups(model, weight_decay=weight_decay),\n",
        "                 lr=lr, betas=betas, eps=eps, fused=fused_ok)\n",
        "\n",
        "def build_smooth_cosine_scheduler(optimizer, warmup_steps, total_steps, min_lr_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Smooth cosine warmup (cosine from 0->1) followed by cosine decay (1->0),\n",
        "    scaled to finish at min_lr_ratio * base_lr. No clamping/kinks.\n",
        "    \"\"\"\n",
        "    assert total_steps > 0\n",
        "    min_ratio = float(min_lr_ratio)\n",
        "\n",
        "    def lr_lambda(step):\n",
        "        # Normalize progress to [0, 1]\n",
        "        s = float(step)\n",
        "        T = float(total_steps)\n",
        "        if s < warmup_steps and warmup_steps > 0:\n",
        "            # cosine warmup: 0 -> 1 smoothly\n",
        "            w = (s + 1.0) / warmup_steps\n",
        "            warm = 0.5 * (1.0 - math.cos(math.pi * min(w, 1.0)))\n",
        "            return warm  # scales from 0 -> 1\n",
        "        # cosine decay from 1 -> min_ratio\n",
        "        # progress p goes from 0 at end of warmup to 1 at final step\n",
        "        denom = max(1.0, T - max(1.0, float(warmup_steps)))\n",
        "        p = (s - warmup_steps) / denom\n",
        "        p = max(0.0, min(1.0, p))\n",
        "        cosine = 0.5 * (1.0 + math.cos(math.pi * p))  # 1 -> 0\n",
        "        # Affine shift: land exactly at min_ratio when p=1\n",
        "        return min_ratio + (1.0 - min_ratio) * cosine\n",
        "\n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# ---- size your schedule ----\n",
        "steps_per_epoch = max(1, len(lines) // batch_size)\n",
        "total_steps = epochs * steps_per_epoch\n",
        "warmup_steps = max(1, int(0.03 * total_steps))  # 3% warmup is usually enough\n",
        "\n",
        "optimizer = build_adamw(gpt, lr=base_lr, weight_decay=0.1)\n",
        "scheduler = build_smooth_cosine_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    total_steps=total_steps,\n",
        "    min_lr_ratio=0.10,      # floor at 10% of base_lr (e.g., 1e-5 if base=1e-4)\n",
        ")\n",
        "\n",
        "print(f\"AdamW ready | fused={getattr(optimizer, 'fused', False)}\")\n",
        "print(f\"steps/epoch={steps_per_epoch}, total_steps={total_steps}, warmup_steps={warmup_steps}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b66199",
      "metadata": {
        "id": "52b66199"
      },
      "source": [
        "### Step 7: Begin training (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d4ebeb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d4ebeb4",
        "outputId": "b9b14340-3626-45b5-ca52-f9d3b33a26e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1, Step 1562, Loss: 0.0242, LR: 0.000093: : 1562it [04:17,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2, Step 1562, Loss: 0.0215, LR: 0.000071: : 1562it [04:19,  6.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3, Step 1562, Loss: 0.0189, LR: 0.000043: : 1562it [04:19,  6.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4, Step 1562, Loss: 0.0191, LR: 0.000019: : 1562it [04:19,  6.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5, Step 1562, Loss: 0.0185, LR: 0.000010: : 1562it [04:18,  6.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved checkpoint to ./dpo.pt\n"
          ]
        }
      ],
      "source": [
        "total_steps = len(lines) // batch_size\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(get_batches(lines, batch_size))\n",
        "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
        "        ###########################################################\n",
        "        neg_logprob = compute_logprob(neg_tensor)  # shape: (batch_size,)\n",
        "        pos_logprob = compute_logprob(pos_tensor)  # shape: (batch_size,)\n",
        "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)  # gradient clipping\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_description(f\"Epoch {epoch+1}, Step {step+1}, Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "        ###########################################################\n",
        "    ckpt_path = f\"./dpo.pt\"\n",
        "    torch.save({\n",
        "        \"model_state_dict\": gpt.state_dict(),\n",
        "        \"model_args\": ckpt['model_args'],\n",
        "    }, ckpt_path)\n",
        "    print(f\"Saved checkpoint to {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b7f2ab",
      "metadata": {
        "id": "48b7f2ab"
      },
      "source": [
        "### Step 8: Begin testing (**students are required to complete this part!**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "09027262",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09027262",
        "outputId": "2f9c38c5-3162-4d6f-deac-bc8346ee5f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 17+19=?\n",
            "Answer: 17+19=? The answer is 36 because 17+19 equals 36.\n",
            "\n",
            "Prompt: 3*17=?\n",
            "Answer: 3*17=? The answer is 51 because 3*17 equals 51.\n",
            "\n",
            "Prompt: 72/4=?\n",
            "Answer: 72/4=? The answer is 18 because 72/4 equals 18.\n",
            "\n",
            "Prompt: 72-x=34,x=?\n",
            "Answer: 72-x=34,x=? The answer is 38 because 72-34=388.\n",
            "\n",
            "Prompt: x*11=44,x=?\n",
            "Answer: x*11=44,x=? The answer is 4 because 44/11=4444.\n",
            "\n",
            "Prompt: 3*17=?\n",
            "Answer: 3*17=? The answer is 51 because 3*17 equals 51.\n",
            "\n",
            "Prompt: 72/4=?\n",
            "Answer: 72/4=? The answer is 18 because 72/4 equals 18.\n",
            "\n",
            "Prompt: 72-x=34,x=?\n",
            "Answer: 72-x=34,x=? The answer is 48 because 72-34=488.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the fine-tuned model\n",
        "ckpt_path = \"/content/dpo.pt\"\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "gpt = GPT(gptconf).cuda()\n",
        "try:\n",
        "    state_dict = checkpoint['model']\n",
        "except:\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "gpt.load_state_dict(state_dict)\n",
        "# Test\n",
        "gpt.eval()\n",
        "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
        "with torch.no_grad():\n",
        "    for prompt in test_set:\n",
        "        prompt_ids = encode(prompt)\n",
        "        ###########################################################\n",
        "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        y = gpt.generate(x, max_new_tokens=max_new_tokens,\n",
        "                         temperature=temperature, top_k=top_k)\n",
        "\n",
        "        # flatten tokens and decode\n",
        "        tokens = y[0].view(-1).tolist()\n",
        "        print(f\"Prompt: {prompt}\\nAnswer: {decode(tokens)}\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}